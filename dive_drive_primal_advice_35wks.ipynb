{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dive_drive_primal_advice_35wks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script that copies demand excel files from gdrive into bq tables"
      ],
      "metadata": {
        "id": "YJOVzc0eUe2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports and auth"
      ],
      "metadata": {
        "id": "hAP_5EIOUs04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# print('Authenticated')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5m1CrQv3UqQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declare Variables"
      ],
      "metadata": {
        "id": "2Lm-06W-UcVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'gdrive_folder_id' : '15JayKFd0LHqALJJ46MpcUyjWXNdS8pZB', # Toro templates drive folder\n",
        "    'local_download_path' : '/content/data', # the path to download the gdrive data to e.g. '/content/data'\n",
        "    'sheet_names' : ['HW','TRUG','BUN'], # These are the sheet names for each spreadsheet to read in as pandas dataframe\n",
        "}"
      ],
      "metadata": {
        "id": "r_tVie55SdQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data from gdrive"
      ],
      "metadata": {
        "id": "U-gsqNo1WETj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser(params['local_download_path'])\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n"
      ],
      "metadata": {
        "id": "P5y_Y2phQmtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get file ID's to download >year>mth>wk>demand planning folder>file"
      ],
      "metadata": {
        "id": "u6mUc-msSFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_drive_folder_content(drive_id):\n",
        "  return drive.ListFile({'q': f\"'{drive_id}' in parents\"}).GetList()\n",
        "\n",
        "def get_match_id(drive_id, regex='.*'):\n",
        "  import re\n",
        "  l = []\n",
        "  drive_list = get_drive_folder_content(drive_id=drive_id)\n",
        "  for d in drive_list:\n",
        "    d_match = re.match(regex, d['title'])\n",
        "    if d_match:\n",
        "      l.append(d['id'])\n",
        "  return l\n",
        "\n",
        "def get_match_title(drive_id, regex='.*'):\n",
        "  import re\n",
        "  l = []\n",
        "  drive_list = get_drive_folder_content(drive_id=drive_id)\n",
        "  for d in drive_list:\n",
        "    d_match = re.match(regex, d['title'])\n",
        "    if d_match:\n",
        "      l.append(d['title'])\n",
        "  return l\n"
      ],
      "metadata": {
        "id": "0rbDrFAXENTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_match = get_match_id(drive_id=params['gdrive_folder_id'], regex='^2021$|^2022$|^2023$')\n",
        "month_match = []\n",
        "for x in year_match:\n",
        "  month_match.append(get_match_id(drive_id=x, regex='.*'))\n",
        "\n",
        "week_match = []\n",
        "for x in month_match:\n",
        "  for iv in x:\n",
        "    week_match.append(get_match_id(drive_id=iv, regex='.*'))\n",
        "\n",
        "demandPlanning_folders = []\n",
        "for x in week_match:\n",
        "  for iv in x:\n",
        "    demandPlanning_folders.append(get_match_id(drive_id=iv, regex='^Demand Planning$'))\n",
        "\n",
        "files_2_dl = []\n",
        "files_2_dl_fnames = []\n",
        "for x in demandPlanning_folders:\n",
        "  for iv in x:\n",
        "    files_2_dl.append(get_match_id(drive_id=iv, regex='^35 week Primal Advice [0-9]{2}-[0-9]{2}-[0-9]{4} .*xlsx$'))\n",
        "    files_2_dl_fnames.append(get_match_title(drive_id=iv, regex='^35 week Primal Advice [0-9]{2}-[0-9]{2}-[0-9]{4} .*xlsx$'))\n",
        "\n",
        "flatten_files_2_dl = [item for sublist in files_2_dl for item in sublist]\n",
        "flatten_files_2_dl_fnames = [item for sublist in files_2_dl_fnames for item in sublist]\n",
        "\n",
        "for t,i in zip(flatten_files_2_dl_fnames,flatten_files_2_dl):\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (t, i))\n",
        "  fname = os.path.join(local_download_path, t)\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': i})\n",
        "  f_.GetContentFile(fname)"
      ],
      "metadata": {
        "id": "ujCgp7JsL7LS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2551a38d-4d2e-492a-d76b-94f6d8486817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: 35 week Primal Advice 23-03-2022 with unlimited capasity (1).xlsx, id: 1vq1J-nxG2oBe8mZYC96cZb4LZf10xeui\n",
            "downloading to /content/data/35 week Primal Advice 23-03-2022 with unlimited capasity (1).xlsx\n",
            "title: 35 week Primal Advice 19-03-2022 with unlimited capasity.xlsx, id: 1hxmmEssH0JzjbutCj8y1jkufuA6u-619\n",
            "downloading to /content/data/35 week Primal Advice 19-03-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 11-03-2022 with unlimited capasity.xlsx, id: 1hBhRhiJ2JvHDLo_3uVgsK66Y5pvUlE3y\n",
            "downloading to /content/data/35 week Primal Advice 11-03-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 03-03-2022 with unlimited capasity.xlsx, id: 1RBzUuA0KX-xkBpCCnCbZW2OJHgejD-2f\n",
            "downloading to /content/data/35 week Primal Advice 03-03-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 25-02-2022 with unlimited capasity.xlsx, id: 1YqryttwDQiZguGUylLSTY_Xmh5s9_d0S\n",
            "downloading to /content/data/35 week Primal Advice 25-02-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 18-02-2022 with unlimited capasity.xlsx, id: 1Nr2UkdVjAZe3qDnspjnH_lXUlZyTWoCq\n",
            "downloading to /content/data/35 week Primal Advice 18-02-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 11-02-2022 with unlimited capasity.xlsx, id: 1t97G9QJnrkoFfRJj8NlQvPwKIXwxMVjE\n",
            "downloading to /content/data/35 week Primal Advice 11-02-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 04-02-2022 with unlimited capasity.xlsx, id: 1aQeaVjdO4C62U7hIb-GemYRyr53SE_HE\n",
            "downloading to /content/data/35 week Primal Advice 04-02-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 28-01-2022 with unlimited capasity.xlsx, id: 1EVt9T1NwpxR-tw3X2DQAWVSuuwSf_5Yh\n",
            "downloading to /content/data/35 week Primal Advice 28-01-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 21-01-2022 with unlimited capasity.xlsx, id: 1HexWWL63PFY06SzjqPAZGByRoK000Caw\n",
            "downloading to /content/data/35 week Primal Advice 21-01-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 13-01-2022 with unlimited capasity.xlsx, id: 11t1ox1riLUjPbGcyyzJs2EiyhpmA7etO\n",
            "downloading to /content/data/35 week Primal Advice 13-01-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx, id: 1_CiuLuYe_uyUaHo89cKn3w60CEGMZCze\n",
            "downloading to /content/data/35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 17-12-2021 with unlimited capasity.xlsx, id: 1Eg9sJByIeEEOQLcAgRuPMeSkeptghdbW\n",
            "downloading to /content/data/35 week Primal Advice 17-12-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 22-12-2021 with unlimited capasity.xlsx, id: 1_49jPYJAwFDX9FFOSbCoKSdrNWj8HAQ2\n",
            "downloading to /content/data/35 week Primal Advice 22-12-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 09-12-2021 with unlimited capasity.xlsx, id: 16sOHAkM2ESPAoWXptKcM9IHRP4RXBb7G\n",
            "downloading to /content/data/35 week Primal Advice 09-12-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 02-12-2021 with unlimited capasity.xlsx, id: 1QViduoe1OsiZ7TzWDhzWMiG-OV4KcL3i\n",
            "downloading to /content/data/35 week Primal Advice 02-12-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 26-11-2021 with unlimited capasity.xlsx, id: 13Yop8w_ANOPUzVni9vWAG3d6VdKL8Hgz\n",
            "downloading to /content/data/35 week Primal Advice 26-11-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 22-11-2021 with unlimited capasity.xlsx, id: 1lpQrevXiHH82Md5y9SIKDFDpc1f_SC6m\n",
            "downloading to /content/data/35 week Primal Advice 22-11-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 11-11-2021 with unlimited capasity.xlsx, id: 1ER3jr4Uwms4IcEgsNgNeCwOtFY1x-g0Z\n",
            "downloading to /content/data/35 week Primal Advice 11-11-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 05-11-2021 with unlimited capasity.xlsx, id: 1DrQPAXHf8oMTMpcXHTl6iDDFHq5Ye2yf\n",
            "downloading to /content/data/35 week Primal Advice 05-11-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 22-10-2021 with unlimited capasity.xlsx, id: 1zvtfHfW8KqaWQCEpU2TpEL9kZOTCnykL\n",
            "downloading to /content/data/35 week Primal Advice 22-10-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 15-10-2021 with unlimited capasity.xlsx, id: 1oRemvxlrdfgqkFP2pEYIqH8aYHdGciat\n",
            "downloading to /content/data/35 week Primal Advice 15-10-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 06-10-2021 with unlimited capasity.xlsx, id: 1kBKdpPio121KhiObvGTClmJkU0gz3daf\n",
            "downloading to /content/data/35 week Primal Advice 06-10-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 29-09-2021 with unlimited capasity.xlsx, id: 17CFKiXmMdT9ZD53kUNxhuKHphDwa1HjW\n",
            "downloading to /content/data/35 week Primal Advice 29-09-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 23-09-2021 with unlimited capasity.xlsx, id: 1L2Yjv6a2PGD-C4yRyQXFS1k3an7Tv03K\n",
            "downloading to /content/data/35 week Primal Advice 23-09-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 15-09-2021 with unlimited capasity.xlsx, id: 1o5y-jAHN3l-CWH65lk_8gd9-GJbUtkbo\n",
            "downloading to /content/data/35 week Primal Advice 15-09-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 09-09-2021 with unlimited capasity.xlsx, id: 1vctmM7YTuejnCHwdL_5qww77JrSlBvdz\n",
            "downloading to /content/data/35 week Primal Advice 09-09-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 02-09-2021 with unlimited capasity.xlsx, id: 1JAi0xEIPAb9wkvWyOh5RG2ejXXjxJg3U\n",
            "downloading to /content/data/35 week Primal Advice 02-09-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 26-08-2021 with unlimited capasity.xlsx, id: 1kxpIbKxlt-vPkTrrzkU-phA6mTR96655\n",
            "downloading to /content/data/35 week Primal Advice 26-08-2021 with unlimited capasity.xlsx\n",
            "title: 35 week Primal Advice 12-08-2021 with unlimited capasity (1).xlsx, id: 1q-13q2rgTreMy-KYQMxTnZN0tsFEs6Ln\n",
            "downloading to /content/data/35 week Primal Advice 12-08-2021 with unlimited capasity (1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### from excel to pandas dataframes"
      ],
      "metadata": {
        "id": "CfXtPoqGWL9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {}\n",
        "for f in os.listdir(params['local_download_path']):\n",
        "  f_path = params['local_download_path']+'/'+f\n",
        "  df_dict[f] = pd.read_excel(f_path, sheet_name=params['sheet_names'])\n"
      ],
      "metadata": {
        "id": "DfSLLtP7WPbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add columns to each dataframe"
      ],
      "metadata": {
        "id": "UIHyLEvzjUE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_dict['35 week Primal Advice 09-09-2021 with unlimited capasity.xlsx']#['HW']#.describe()\n",
        "# for each sheet of data in excel file (now dict of dataframes in dict where key = filename and subkey = sheetName)\n",
        "# add a column for sheet name e.g. HW, TRUG, BUN\n",
        "\n",
        "def get_dmy_from_filename(fn):\n",
        "  title_search = re.search('([0-9]{2})-([0-9]{2})-([0-9]{4})', fn, re.IGNORECASE)\n",
        "  if title_search:\n",
        "    return title_search.group(3)+'-'+title_search.group(2)+'-'+title_search.group(1)\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "for ss in df_dict.keys():\n",
        "  for sname in df_dict[ss].keys():\n",
        "    df_dict[ss][sname]['sheet_name'] = sname\n",
        "    df_dict[ss][sname]['source_date'] = get_dmy_from_filename(ss)\n",
        "    df_dict[ss][sname]['file_name'] = ss\n"
      ],
      "metadata": {
        "id": "L_KvQPRLWlsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make single dataframe"
      ],
      "metadata": {
        "id": "EUylcAymjXty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_df(dict_of_df):\n",
        "  return pd.concat(dict_of_df).reset_index().drop(columns=['level_0','level_1'])\n",
        "\n",
        "all_df = pd.concat([stack_df(df_dict[ss]) for ss in df_dict.keys()])"
      ],
      "metadata": {
        "id": "EnGA9XHTjbwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean column names"
      ],
      "metadata": {
        "id": "u481ZKcPopiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns = all_df.columns.str.replace(' ','_', regex=False)\n",
        "all_df.columns = all_df.columns.str.replace('.','_', regex=False)"
      ],
      "metadata": {
        "id": "HD3Cx9iKov_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns"
      ],
      "metadata": {
        "id": "HEmp0okpr-X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reorder columns"
      ],
      "metadata": {
        "id": "OrGe01HuLCAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stefan Petiq 20220214:\n",
        "# Everything exactly how it is except\n",
        "# Wow_code\n",
        "# Plant\n",
        "# Sheet_name\n",
        "# Source_date\n",
        "# file_name\n",
        "# moved to be the last 5 fields at the end of the sheet\n",
        "\n",
        "last_cols = ['WOW_code', 'Plant', 'sheet_name', 'source_date', 'file_name']\n",
        "new_col_order = [x for x in all_df.columns if x not in last_cols]+last_cols\n",
        "\n",
        "all_df = all_df[new_col_order]"
      ],
      "metadata": {
        "id": "uJKvKYgXLD0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete unwanted sheet"
      ],
      "metadata": {
        "id": "-ErMwBeVLo3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete 'week' from the \"35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx\" file so it should not appear in the data loading anymore\n",
        "all_df = all_df.query(\"file_name != '35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx'\")"
      ],
      "metadata": {
        "id": "YKIvDu1vLogv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### change dtypes"
      ],
      "metadata": {
        "id": "Q83S0qTatRiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns"
      ],
      "metadata": {
        "id": "Rl05yOuEvbuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.drop(columns='week', inplace=True)"
      ],
      "metadata": {
        "id": "FmLB3EQlXwCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save to bigquery"
      ],
      "metadata": {
        "id": "xzo6LPA8mt2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def float2str(d):\n",
        "  return ['' if np.isnan(x) else str(int(x)) for x in d]\n",
        "all_df['WOW_code'] = float2str(all_df['WOW_code'])\n",
        "all_df['PrimalID'] = float2str(all_df['PrimalID'])"
      ],
      "metadata": {
        "id": "2KzPw3zXtS84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.io.gbq.to_gbq(all_df, 'hilton.hilton_primaladvice_35weeks', 'gcp-wow-pvc-grnstck-prod', chunksize=100000, reauth=False, if_exists='replace')"
      ],
      "metadata": {
        "id": "oadte9r7ldKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
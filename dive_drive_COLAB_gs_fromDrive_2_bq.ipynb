{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dive_drive_COLAB_gs_fromDrive_2_bq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script that copies demand excel files from gdrive into bq tables"
      ],
      "metadata": {
        "id": "YJOVzc0eUe2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports and auth"
      ],
      "metadata": {
        "id": "hAP_5EIOUs04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# print('Authenticated')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5m1CrQv3UqQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declare Variables"
      ],
      "metadata": {
        "id": "2Lm-06W-UcVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'gdrive_folder_id' : '15JayKFd0LHqALJJ46MpcUyjWXNdS8pZB', #'1FumjvqmkvENsulhJzOQvNN5KGX7EeVNn', # the ID of gdrive folder to get data from\n",
        "    'local_download_path' : '/content/data', # the path to download the gdrive data to e.g. '/content/data'\n",
        "    'sheet_names' : ['HW','TRUG','BUN'], # These are the sheet names for each spreadsheet to read in as pandas dataframe\n",
        "}"
      ],
      "metadata": {
        "id": "r_tVie55SdQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data from gdrive"
      ],
      "metadata": {
        "id": "U-gsqNo1WETj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser(params['local_download_path'])\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n"
      ],
      "metadata": {
        "id": "P5y_Y2phQmtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get file ID's to download >year>mth>wk>demand planning folder>file"
      ],
      "metadata": {
        "id": "u6mUc-msSFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_drive_folder_content(drive_id):\n",
        "  return drive.ListFile({'q': f\"'{drive_id}' in parents\"}).GetList()\n",
        "\n",
        "def get_match_id(drive_id, regex='.*'):\n",
        "  import re\n",
        "  l = []\n",
        "  drive_list = get_drive_folder_content(drive_id=drive_id)\n",
        "  for d in drive_list:\n",
        "    d_match = re.match(regex, d['title'])\n",
        "    if d_match:\n",
        "      l.append(d['id'])\n",
        "  return l\n",
        "\n",
        "def get_match_title(drive_id, regex='.*'):\n",
        "  import re\n",
        "  l = []\n",
        "  drive_list = get_drive_folder_content(drive_id=drive_id)\n",
        "  for d in drive_list:\n",
        "    d_match = re.match(regex, d['title'])\n",
        "    if d_match:\n",
        "      l.append(d['title'])\n",
        "  return l\n"
      ],
      "metadata": {
        "id": "0rbDrFAXENTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_match = get_match_id(drive_id=params['gdrive_folder_id'], regex='^2021$|^2022$|^2023$')\n",
        "month_match = []\n",
        "for x in year_match:\n",
        "  month_match.append(get_match_id(drive_id=x, regex='.*'))\n",
        "\n",
        "week_match = []\n",
        "for x in month_match:\n",
        "  for iv in x:\n",
        "    week_match.append(get_match_id(drive_id=iv, regex='.*'))\n",
        "\n",
        "demandPlanning_folders = []\n",
        "for x in week_match:\n",
        "  for iv in x:\n",
        "    demandPlanning_folders.append(get_match_id(drive_id=iv, regex='^Demand Planning$'))\n",
        "\n",
        "files_2_dl = []\n",
        "files_2_dl_fnames = []\n",
        "for x in demandPlanning_folders:\n",
        "  for iv in x:\n",
        "    files_2_dl.append(get_match_id(drive_id=iv, regex='^35 week Primal Advice [0-9]{2}-[0-9]{2}-[0-9]{4} .*xlsx$'))\n",
        "    files_2_dl_fnames.append(get_match_title(drive_id=iv, regex='^35 week Primal Advice [0-9]{2}-[0-9]{2}-[0-9]{4} .*xlsx$'))\n",
        "\n",
        "flatten_files_2_dl = [item for sublist in files_2_dl for item in sublist]\n",
        "flatten_files_2_dl_fnames = [item for sublist in files_2_dl_fnames for item in sublist]\n",
        "\n",
        "for t,i in zip(flatten_files_2_dl_fnames,flatten_files_2_dl):\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (t, i))\n",
        "  fname = os.path.join(local_download_path, t)\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': i})\n",
        "  f_.GetContentFile(fname)"
      ],
      "metadata": {
        "id": "ujCgp7JsL7LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### from excel to pandas dataframes"
      ],
      "metadata": {
        "id": "CfXtPoqGWL9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {}\n",
        "for f in os.listdir(params['local_download_path']):\n",
        "  f_path = params['local_download_path']+'/'+f\n",
        "  df_dict[f] = pd.read_excel(f_path, sheet_name=params['sheet_names'])\n"
      ],
      "metadata": {
        "id": "DfSLLtP7WPbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add columns to each dataframe"
      ],
      "metadata": {
        "id": "UIHyLEvzjUE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_dict['35 week Primal Advice 09-09-2021 with unlimited capasity.xlsx']#['HW']#.describe()\n",
        "# for each sheet of data in excel file (now dict of dataframes in dict where key = filename and subkey = sheetName)\n",
        "# add a column for sheet name e.g. HW, TRUG, BUN\n",
        "\n",
        "def get_dmy_from_filename(fn):\n",
        "  title_search = re.search('([0-9]{2})-([0-9]{2})-([0-9]{4})', fn, re.IGNORECASE)\n",
        "  if title_search:\n",
        "    return title_search.group(3)+'-'+title_search.group(2)+'-'+title_search.group(1)\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "for ss in df_dict.keys():\n",
        "  for sname in df_dict[ss].keys():\n",
        "    df_dict[ss][sname]['sheet_name'] = sname\n",
        "    df_dict[ss][sname]['source_date'] = get_dmy_from_filename(ss)\n",
        "    df_dict[ss][sname]['file_name'] = ss\n"
      ],
      "metadata": {
        "id": "L_KvQPRLWlsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make single dataframe"
      ],
      "metadata": {
        "id": "EUylcAymjXty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_df(dict_of_df):\n",
        "  return pd.concat(dict_of_df).reset_index().drop(columns=['level_0','level_1'])\n",
        "\n",
        "all_df = pd.concat([stack_df(df_dict[ss]) for ss in df_dict.keys()])"
      ],
      "metadata": {
        "id": "EnGA9XHTjbwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean column names"
      ],
      "metadata": {
        "id": "u481ZKcPopiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns = all_df.columns.str.replace(' ','_', regex=False)\n",
        "all_df.columns = all_df.columns.str.replace('.','_', regex=False)"
      ],
      "metadata": {
        "id": "HD3Cx9iKov_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEmp0okpr-X6",
        "outputId": "4ca1fcc4-b69c-4b12-a56f-05a09d6605c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'PrimalID', 'Family', 'PrimalName', 'Primal_kgs_for_production',\n",
              "       'Proposed_Purchases', 'Confirmed_Purchases', 'Low_Codes', 'Op_Stock',\n",
              "       'Unmature_stock', 'Butcher_shop_Rq_', 'WOW_code', 'Plant', 'sheet_name',\n",
              "       'source_date', 'file_name', 'StockInDays', 'week', 'Target_inventory',\n",
              "       'TargetInDays'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reorder columns"
      ],
      "metadata": {
        "id": "OrGe01HuLCAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stefan Petiq 20220214:\n",
        "# Everything exactly how it is except\n",
        "# Wow_code\n",
        "# Plant\n",
        "# Sheet_name\n",
        "# Source_date\n",
        "# file_name\n",
        "# moved to be the last 5 fields at the end of the sheet\n",
        "\n",
        "last_cols = ['WOW_code', 'Plant', 'sheet_name', 'source_date', 'file_name']\n",
        "new_col_order = [x for x in all_df.columns if x not in last_cols]+last_cols\n",
        "\n",
        "all_df = all_df[new_col_order]"
      ],
      "metadata": {
        "id": "uJKvKYgXLD0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete unwanted sheet"
      ],
      "metadata": {
        "id": "-ErMwBeVLo3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete 'week' from the \"35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx\" file so it should not appear in the data loading anymore\n",
        "all_df = all_df.query(\"file_name != '35 week Primal Advice 10-01-2022 with unlimited capasity.xlsx'\")"
      ],
      "metadata": {
        "id": "YKIvDu1vLogv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### change dtypes"
      ],
      "metadata": {
        "id": "Q83S0qTatRiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl05yOuEvbuq",
        "outputId": "ce676b60-c57c-48a8-9d3f-2d8e953850fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'PrimalID', 'Family', 'PrimalName', 'Primal_kgs_for_production',\n",
              "       'Proposed_Purchases', 'Confirmed_Purchases', 'Low_Codes', 'Op_Stock',\n",
              "       'Unmature_stock', 'Butcher_shop_Rq_', 'StockInDays', 'week',\n",
              "       'Target_inventory', 'TargetInDays', 'WOW_code', 'Plant', 'sheet_name',\n",
              "       'source_date', 'file_name'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.drop(columns='week', inplace=True)"
      ],
      "metadata": {
        "id": "FmLB3EQlXwCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save to bigquery"
      ],
      "metadata": {
        "id": "xzo6LPA8mt2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def float2str(d):\n",
        "  return ['' if np.isnan(x) else str(int(x)) for x in d]\n",
        "all_df['WOW_code'] = float2str(all_df['WOW_code'])\n",
        "all_df['PrimalID'] = float2str(all_df['PrimalID'])"
      ],
      "metadata": {
        "id": "2KzPw3zXtS84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.io.gbq.to_gbq(all_df, 'masterdata_view.hfa_primal_advice', 'gcp-wow-pvc-grnstck-prod', chunksize=100000, reauth=False, if_exists='replace')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oadte9r7ldKH",
        "outputId": "d6f7920a-29d5-4012-98cf-bebfc981077e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [02:21, 10.90s/it]\n"
          ]
        }
      ]
    }
  ]
}